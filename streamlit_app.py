# -*- coding: utf-8 -*-
# ì‹¤í–‰: streamlit run streamlit_app.py --server.port 8501 --server.address 0.0.0.0
import io
import os
from base64 import b64encode
import numpy as np
import pandas as pd
import requests
import streamlit as st

from pyecharts.charts import Line
from pyecharts import options as opts
from pyecharts.globals import ThemeType
from streamlit_echarts import st_pyecharts

# -------------------------------------------------
# ê¸°ë³¸ ì„¤ì • + í°íŠ¸ ì£¼ì…
# -------------------------------------------------
st.set_page_config(page_title="í•´ìˆ˜ë©´ ìƒìŠ¹ ëŒ€ì‹œë³´ë“œ (ECharts)", layout="wide", page_icon="ğŸŒŠ")

def inject_font_css(font_path="/fonts/Pretendard-Bold.ttf", family="Pretendard"):
    if not os.path.exists(font_path):
        return
    with open(font_path, "rb") as f:
        font_data = b64encode(f.read()).decode("utf-8")
    css = f"""
    <style>
    @font-face {{
        font-family: '{family}';
        src: url(data:font/ttf;base64,{font_data}) format('truetype');
        font-weight: 700; font-style: normal; font-display: swap;
    }}
    html, body, [class*="css"] {{
        font-family: '{family}', -apple-system, BlinkMacSystemFont, "Segoe UI",
                     Roboto, "Helvetica Neue", Arial, "Noto Sans KR", "Apple SD Gothic Neo",
                     "Nanum Gothic", "Malgun Gothic", sans-serif !important;
    }}
    </style>
    """
    st.markdown(css, unsafe_allow_html=True)

inject_font_css("/fonts/Pretendard-Bold.ttf", family="Pretendard")

# -------------------------------------------------
# í—¤ë”
# -------------------------------------------------
st.title("ğŸŒŠ ì „ì§€êµ¬ í‰ê·  í•´ìˆ˜ë©´(GMSL) â€” 1900s â†’ 2025")
st.caption("ì¡°ìœ„ê³„ ì¬êµ¬ì„±(CSIRO) + ìœ„ì„± ê³ ë„ê³„(NOAA STAR) Â· 1900ë…„ ê¸°ì¤€ 0 ì¬ì •ë ¬ + 1993â€“2010 í‰ê· ì„  í‘œì‹œ")

with st.expander("ğŸ“ ë°ì´í„° ì†ŒìŠ¤ ì•ˆë‚´"):
    st.markdown(
        """
- **ì¥ê¸° ì‹œê³„ì—´(1880~2009)**: CSIRO ì¡°ìœ„ê³„ ê¸°ë°˜ ì¬êµ¬ì„± (DataHub CSV)
- **ìœ„ì„± ì‹œê¸°(1993~í˜„ì¬)**: NOAA NESDIS/STAR ìµœì‹  CSV 2ì¢…(ì—°ì£¼ê¸° ìœ ì§€/ì œê±°) ì¤‘ ì„±ê³µí•˜ëŠ” ê²ƒ ìë™ ì„ íƒ
- ë„¤íŠ¸ì›Œí¬/URL ë³€ê²½ ì‹œë¥¼ ëŒ€ë¹„í•´ ë°ëª¨ ë°ì´í„°ë¥¼ ìë™ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
    )

# -------------------------------------------------
# ì›ê²© CSV ë¡œë” & íŒŒì„œ
# -------------------------------------------------
def fetch_csv_from_candidates(candidates, **read_csv_kwargs) -> pd.DataFrame:
    last_err = None
    for url in candidates:
        try:
            r = requests.get(url, timeout=25)
            r.raise_for_status()
            df = pd.read_csv(io.BytesIO(r.content), **read_csv_kwargs) if read_csv_kwargs else pd.read_csv(io.BytesIO(r.content))
            df["__source_url__"] = url
            return df
        except Exception as e:
            last_err = e
    if last_err:
        raise last_err
    raise RuntimeError("No URL candidates provided")

def read_noaa_altimetry_csv(url: str) -> pd.DataFrame:

    """
    NOAA STAR CSV(ì£¼ì„/ê°€ë³€í¬ë§·) â†’ í‘œì¤€í™”:
      - ë‚ ì§œ: (year, month) ë˜ëŠ” (decimal year) ë˜ëŠ” ë¬¸ìì—´ ë‚ ì§œ ìë™ ì²˜ë¦¬
      - ê°’ì—´: ë’¤ìª½ ì—´ë¶€í„° ìˆ«ì ë¹„ìœ¨ ë†’ì€ ì—´ ì„ íƒ
      - NaN/NaT ì•ˆì „ ê°€ë“œ (ì •ìˆ˜ ë³€í™˜ì€ dropna ì´í›„ì—ë§Œ!)
    """
    r = requests.get(url, timeout=25)
    r.raise_for_status()
    txt = r.text

    # 1) ì£¼ì„/ë¹ˆì¤„ ì œê±°
    lines = [ln for ln in txt.splitlines() if ln.strip() and not ln.lstrip().startswith("#")]
    if not lines:
        raise ValueError("NOAA CSV: ìœ íš¨í•œ ë°ì´í„° ì¤„ì´ ì—†ìŠµë‹ˆë‹¤.")
    raw = "\n".join(lines)

    # 2) ìœ ì—° íŒŒì‹±
    df = pd.read_csv(
        io.StringIO(raw),
        sep=r"[,\s]+",
        engine="python",
        header=None,
        comment="#",
        skip_blank_lines=True
    ).dropna(axis=1, how="all")

    if df.shape[1] < 2:
        raise ValueError("NOAA CSV: ì—´ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

    c0 = df.iloc[:, 0]
    c1 = df.iloc[:, 1] if df.shape[1] > 1 else None

    def decyear_to_datetime(y):
        # NaN/inf ê°€ë“œ
        try:
            if pd.isna(y):
                return pd.NaT
            y = float(y)
            if not np.isfinite(y):
                return pd.NaT
        except Exception:
            return pd.NaT
        year = int(np.floor(y))
        rem = y - year
        return pd.Timestamp(year, 1, 1) + pd.to_timedelta(rem * 365.25, unit="D")

    d = None

        # (A) year, month í˜•íƒœ (ë‘˜ ë‹¤ ìˆ˜ì¹˜ + ìœ íš¨ ë²”ìœ„)
    c0_num = pd.to_numeric(c0, errors="coerce")
    c1_num = pd.to_numeric(c1, errors="coerce") if c1 is not None else None
    if c1_num is not None:
        y_ok = c0_num.between(1800, 2100)
        m_ok = c1_num.between(1, 12)

        # âœ… ì—°Â·ì›”ì´ â€œë™ì‹œì— ìœ íš¨â€í•˜ê³  NaNì´ ì•„ë‹Œ í–‰ë§Œ ì—„ê²© í•„í„°ë§
        mask = y_ok & m_ok & c0_num.notna() & c1_num.notna()
        frac = mask.mean()

        if frac > 0.9:
            # ìœ íš¨ í–‰ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©í•˜ì—¬ â€œì •ìˆ˜ ìºìŠ¤íŒ…â€
            idx = mask[mask].index
            years = c0_num.loc[idx].astype("int64")
            months = c1_num.loc[idx].astype("int64")

            # ì•ˆì „í•˜ê²Œ 1ì¼ ê³ ì •
            d_conv = pd.to_datetime(
                dict(year=years, month=months, day=np.ones(len(idx), dtype="int64")),
                errors="coerce"
            )

            # ì›ë³¸ ì¸ë±ìŠ¤ì— ë§ì¶° NaTë¡œ ì±„ì›Œë‘ê³  ìœ íš¨ ìœ„ì¹˜ë§Œ ëŒ€ì…
            d_full = pd.Series(pd.NaT, index=df.index)
            d_full.loc[idx] = d_conv
            d = d_full

            # ì´í›„ ê°’ ì—´ í›„ë³´ëŠ” 2ì—´ë¶€í„°
            value_candidates = list(range(2, df.shape[1]))


    # (B) ì†Œìˆ˜ ì—°ë„
    if d is None:
        if c0_num.notna().mean() > 0.9 and c0_num.between(1800, 2100).mean() > 0.9:
            d = c0_num.map(decyear_to_datetime)

    # (C) ë¬¸ìì—´ ë‚ ì§œ
    if d is None:
        d_try = pd.to_datetime(c0, errors="coerce")
        if d_try.notna().mean() > 0.9:
            d = d_try

    if d is None:
        raise ValueError("NOAA CSV: ë‚ ì§œ ì—´ì„ í•´ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ê°’ ì—´: ë’¤ì—ì„œ ì•ìœ¼ë¡œ ìˆ«ì ë¹„ìœ¨ ë†’ì€ ì—´ ì‚¬ìš©
    valcol = None
    for col in reversed(range(1, df.shape[1])):
        col_vals = pd.to_numeric(df.iloc[:, col], errors="coerce")
        if col_vals.notna().mean() > 0.7:
            valcol = col
            break
    if valcol is None:
        raise ValueError("NOAA CSV ê°’ ì—´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    out = pd.DataFrame({
        "date": pd.to_datetime(d, errors="coerce"),
        "gmsl_mm": pd.to_numeric(df.iloc[:, valcol], errors="coerce")
    }).dropna(subset=["date", "gmsl_mm"]).sort_values("date")

    # ì˜¤ëŠ˜(ì„œìš¸) ì´í›„ ë°ì´í„° ì»·
    today_seoul = pd.Timestamp.now(tz="Asia/Seoul").normalize().tz_localize(None)
    out = out[out["date"] <= today_seoul]

    out["__source_url__"] = url
    return out


# -------------------------------------------------
# ë°ì´í„° ì†ŒìŠ¤ URL
# -------------------------------------------------
CSIRO_CANDIDATES = [
    # DataHub: CSIRO ì¬êµ¬ì„±(1880â€“2009) ì›”ë³„ (Time, GMSL, GMSL uncertainty)
    "https://datahub.io/core/sea-level-rise/r/csiro_recons_gmsl_mo_2015.csv"
]
ALTIMETRY_CANDIDATES = [
    # NOAA STAR ìµœì‹ (ì—°ì£¼ê¸° ìœ ì§€)
    "https://www.star.nesdis.noaa.gov/socd/lsa/SeaLevelRise/slr/slr_sla_gbl_keep_ref_90.csv",
    # NOAA STAR ìµœì‹ (ì—°ì£¼ê¸° ì œê±°)
    "https://www.star.nesdis.noaa.gov/socd/lsa/SeaLevelRise/slr/slr_sla_gbl_free_all_66.csv",
]

# -------------------------------------------------
# ë°ëª¨ ë°±ì—…
# -------------------------------------------------
def demo_fallback() -> tuple[pd.DataFrame, pd.DataFrame]:
    # ì˜¤ëŠ˜(ì„œìš¸) ê¸°ì¤€ê¹Œì§€ë§Œ ìƒì„±
    today_seoul = pd.Timestamp.now(tz="Asia/Seoul").normalize().tz_localize(None)

    # 1) CSIRO êµ¬ê°„: 1900-01 ~ 1992-12 (ë‹¨, ì˜¤ëŠ˜ë³´ë‹¤ ë¯¸ë˜ë©´ ì˜¤ëŠ˜ë¡œ ì»·)
    end_a = min(today_seoul, pd.Timestamp("1992-12-01"))
    dates_a = pd.date_range("1900-01-01", end_a, freq="MS")
    mm_a = np.linspace(0, 120, len(dates_a)) + np.random.normal(0, 2, len(dates_a))
    df_a = pd.DataFrame(
        {"date": dates_a, "gmsl_mm": mm_a, "__source_url__": "DEMO: CSIRO-like"}
    )

    # 2) ìœ„ì„± êµ¬ê°„: 1993-01 ~ ì˜¤ëŠ˜(ì„œìš¸) (ìµœëŒ€ 2025-12ë¡œ ìƒí•œ, í•˜ì§€ë§Œ ë³´í†µì€ ì˜¤ëŠ˜ë¡œ ì»·)
    start_b = pd.Timestamp("1993-01-01")
    end_b = min(today_seoul, pd.Timestamp("2025-12-01"))
    if end_b < start_b:
        # ê·¹ë‹¨ ìƒí™© ë°©ì–´: ìœ„ì„± êµ¬ê°„ì´ ì•„ì§ ì‹œì‘ ì „ì¸ ê²½ìš° ë¹ˆ í”„ë ˆì„
        df_b = pd.DataFrame(columns=["date", "gmsl_mm", "__source_url__"])
    else:
        dates_b = pd.date_range(start_b, end_b, freq="MS")
        last_base = float(mm_a[-1]) if len(mm_a) else 0.0
        mm_b = last_base + np.linspace(0, 110, len(dates_b)) + np.random.normal(0, 2, len(dates_b))
        df_b = pd.DataFrame(
            {"date": dates_b, "gmsl_mm": mm_b, "__source_url__": "DEMO: NOAA-like"}
        )

    return df_a, df_b
# -------------------------------------------------
# ë¡œë“œ & ê²°í•©
# -------------------------------------------------
@st.cache_data(show_spinner=True, ttl=60*30)
def load_sources():
    c, n = None, None

    # CSIRO ì¬êµ¬ì„±
    try:
        c = fetch_csv_from_candidates(
            CSIRO_CANDIDATES,
            usecols=["Time", "GMSL"],
            parse_dates=["Time"],
        )
        c = c.rename(columns={"Time": "date", "GMSL": "gmsl_mm"})
        c = c[["date", "gmsl_mm", "__source_url__"]].dropna().sort_values("date")
    except Exception as e:
        st.warning(f"CSIRO ì‹¤ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ â†’ ë°ëª¨ ëŒ€ì²´: {e}")
        c = None

    # NOAA ìœ„ì„±
    try:
        last_err = None
        n = None
        for url in ALTIMETRY_CANDIDATES:
            try:
                n = read_noaa_altimetry_csv(url)
                break
            except Exception as e:
                last_err = e
        if n is None:
            raise last_err if last_err else RuntimeError("NOAA candidates failed")
    except Exception as e:
        st.warning(f"ìœ„ì„± ì‹¤ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ â†’ ë°ëª¨ ëŒ€ì²´: {e}")
        n = None

    # Fallback
    if c is None or n is None:
        demo_c, demo_n = demo_fallback()
        c = demo_c if c is None else c
        n = demo_n if n is None else n

    return c, n
def unify_concat(df_a, df_b):
    """1993ë…„ ì´í›„ëŠ” ìœ„ì„±ê°’ ìš°ì„ ìœ¼ë¡œ ê²°í•©"""
    a = df_a.copy(); b = df_b.copy()
    a["date"] = pd.to_datetime(a["date"]); b["date"] = pd.to_datetime(b["date"])
    a["src"] = "ì¡°ìœ„ê³„ ì¬êµ¬ì„± (CSIRO)"
    b["src"] = "ìœ„ì„± ê³ ë„ê³„ (NOAA)"
    start_b = b["date"].min()
    a = a[a["date"] < start_b]
    return pd.concat([a, b], ignore_index=True).sort_values("date")

df_csi, df_noaa = load_sources()
full = unify_concat(df_csi, df_noaa)

# ê²°í•© í›„ ë¯¸ë˜ ë°ì´í„° ì œê±° (ì„œìš¸ ê¸°ì¤€)
_cutoff = pd.Timestamp.now(tz="Asia/Seoul").normalize().tz_localize(None)
full = full[pd.to_datetime(full["date"], errors="coerce") <= _cutoff]
st.cache_data.clear()

# -------------------------------------------------
# ë¡œë“œ & ê²°í•©
# -------------------------------------------------
@st.cache_data(show_spinner=True, ttl=60*30)
def load_sources():
    c, n = None, None

    # CSIRO ì¬êµ¬ì„±(ì›”ë³„)
    try:
        c = fetch_csv_from_candidates(
            CSIRO_CANDIDATES,
            usecols=["Time", "GMSL"],      # DataHub ìŠ¤í‚¤ë§ˆ
            parse_dates=["Time"],
        )
        c = c.rename(columns={"Time": "date", "GMSL": "gmsl_mm"})
        c = c[["date", "gmsl_mm", "__source_url__"]].dropna().sort_values("date")
    except Exception as e:
        st.warning(f"CSIRO ì‹¤ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ â†’ ë°ëª¨ ëŒ€ì²´: {e}")
        c = None

    # ìœ„ì„±(altimetry) â€” NOAA STAR 2ì¢… ì¤‘ ì„±ê³µí•˜ëŠ” ê²ƒ ì‚¬ìš©
    try:
        last_err = None
        n = None
        for url in ALTIMETRY_CANDIDATES:
            try:
                n = read_noaa_altimetry_csv(url)
                break
            except Exception as e:
                last_err = e
        if n is None:
            raise last_err if last_err else RuntimeError("NOAA candidates failed")
    except Exception as e:
        st.warning(f"ìœ„ì„± ì‹¤ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ â†’ ë°ëª¨ ëŒ€ì²´: {e}")
        n = None

    # Fallback
    if c is None or n is None:
        demo_c, demo_n = demo_fallback()
        c = demo_c if c is None else c
        n = demo_n if n is None else n

    return c, n

def unify_concat(df_a, df_b):
    """1993ë…„ ì´í›„ëŠ” ìœ„ì„±ê°’ ìš°ì„ ìœ¼ë¡œ ê²°í•©"""
    a = df_a.copy(); b = df_b.copy()
    a["date"] = pd.to_datetime(a["date"]); b["date"] = pd.to_datetime(b["date"])
    a["src"] = "ì¡°ìœ„ê³„ ì¬êµ¬ì„± (CSIRO)"
    b["src"] = "ìœ„ì„± ê³ ë„ê³„ (NOAA)"
    start_b = b["date"].min()
    a = a[a["date"] < start_b]
    return pd.concat([a, b], ignore_index=True).sort_values("date")

df_csi, df_noaa = load_sources()
full = unify_concat(df_csi, df_noaa)

# -------------------------------------------------
# ì‚¬ì´ë“œë°”(í•œêµ­ì–´)
# -------------------------------------------------
st.sidebar.header("âš™ï¸ ì„¤ì •")
theme = ThemeType.LIGHT  # ê³ ì •(ì›í•˜ë©´ ë¼ë””ì˜¤ë¡œ í† ê¸€ ê°€ëŠ¥)
unit = st.sidebar.radio("ë‹¨ìœ„", ["mm", "inch"], index=0, horizontal=True)
smooth = st.sidebar.checkbox("12ê°œì›” ì´ë™í‰ê· (ìŠ¤ë¬´ë”©)", value=True)
show_markers = st.sidebar.checkbox("í¬ì¸íŠ¸ í‘œì‹œ", value=False)

_dates = pd.to_datetime(full["date"], errors="coerce").dropna()
if _dates.empty:
    st.error("ìœ íš¨í•œ ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ì†ŒìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()
min_year = int(_dates.min().year)
max_year = int(_dates.max().year)
default_start = max(1900, min_year)
default_end = min(2025, max_year)

st.sidebar.markdown("---")
year_range = st.sidebar.slider("í‘œì‹œ ê¸°ê°„(ì—°ë„)", min_value=min_year,
                               max_value=max(2025, max_year),
                               value=(default_start, default_end), step=1)

with st.sidebar.expander("ë°ì´í„° ì†ŒìŠ¤ ìƒíƒœ", expanded=False):
    st.write("CSIRO:", df_csi["__source_url__"].iloc[0])
    st.write("NOAA :", df_noaa["__source_url__"].iloc[0])
    st.write("NOAA ìµœì‹  ì‹œì :", pd.to_datetime(df_noaa["date"]).max().date())

# -------------------------------------------------
# ê°€ê³µ (ìŠ¤ë¬´ë”©/ë‹¨ìœ„ â†’ 1900ë…„ ê¸°ì¤€ 0 ì¬ì •ë ¬ + 1993â€“2010 í‰ê· ì„ )
# -------------------------------------------------
plot_df = full.copy()
plot_df["year"] = plot_df["date"].dt.year
plot_df = plot_df[(plot_df["year"] >= year_range[0]) & (plot_df["year"] <= year_range[1])]
plot_df = plot_df.sort_values("date")

# âœ… NaT ì œê±° (ì—¬ê¸° ì¶”ê°€)
_plot_dates = pd.to_datetime(plot_df["date"], errors="coerce")
plot_df = plot_df.loc[_plot_dates.notna()].copy()
plot_df["date"] = _plot_dates.loc[_plot_dates.notna()]

# ìŠ¤ë¬´ë”©
if smooth:
    plot_df["gmsl_mm_smooth"] = plot_df.groupby("src")["gmsl_mm"].transform(lambda s: s.rolling(12, min_periods=1).mean())
    value_col = "gmsl_mm_smooth"
else:
    value_col = "gmsl_mm"

# ë‹¨ìœ„
if unit == "inch":
    plot_df["value"] = plot_df[value_col] / 25.4
    unit_label = "in"
    unit_label_ko = "ì¸ì¹˜"
else:
    plot_df["value"] = plot_df[value_col]
    unit_label = "mm"
    unit_label_ko = "mm"

# === 1900ë…„ ê¸°ì¤€ 0 ì¬ì •ë ¬ ===
baseline_year = 1900
baseline_mask = plot_df["date"].dt.year == baseline_year
if baseline_mask.any():
    baseline_val = plot_df.loc[baseline_mask, "value"].mean()
else:
    # 1900ë…„ ë°ì´í„°ê°€ ì—†ì„ ë•ŒëŠ” ê°€ì¥ ì´ë¥¸ ì—°ë„ì˜ í‰ê· ì„ ì‚¬ìš©
    first_year = int(plot_df["date"].dt.year.min())
    baseline_val = plot_df.loc[plot_df["date"].dt.year == first_year, "value"].mean()

plot_df["value_adj"] = plot_df["value"] - baseline_val

# === 1993â€“2010 êµ¬ê°„ í‰ê· (ìˆ˜í‰ì„  ìš©) ===
win_start = pd.Timestamp(1993, 1, 1)
win_end   = pd.Timestamp(2010, 12, 31)
mask_9310 = (plot_df["date"] >= win_start) & (plot_df["date"] <= win_end)
avg_9310 = float(plot_df.loc[mask_9310, "value_adj"].mean()) if mask_9310.any() else float(plot_df["value_adj"].mean())

# -------------------------------------------------
# í†µê³„: ë³€í™”ëŸ‰/ì—°í‰ê·  ìƒìŠ¹ë¥  (1900=0 ë³´ì • í›„)
# -------------------------------------------------
def annual_rate(df):
    s = df.sort_values("date")[["date", "value_adj"]].dropna()
    if len(s) < 2: return np.nan, np.nan
    change = s["value_adj"].iloc[-1] - s["value_adj"].iloc[0]
    years = (s["date"].iloc[-1] - s["date"].iloc[0]).days / 365.25
    if years <= 0: return np.nan, np.nan
    return change, change / years

change, rate = annual_rate(plot_df)

# -------------------------------------------------
# ì¶”ì„¸ì„ (ì„ í˜•íšŒê·€) â€” 1900=0 ë³´ì • í›„ ê°’ìœ¼ë¡œ ê³„ì‚°
# -------------------------------------------------
def trend_series(df):
    g = df[["date", "value_adj"]].dropna().sort_values("date")
    if len(g) < 2: return pd.DataFrame(columns=["date","trend"]), np.nan
    year_float = g["date"].dt.year + (g["date"].dt.dayofyear - 1) / 365.25
    p = np.polyfit(year_float.values, g["value_adj"].values, 1)  # slope, intercept
    trend_vals = np.polyval(p, year_float.values)
    return pd.DataFrame({"date": g["date"].values, "trend": trend_vals}), p[0]

trend_df, slope_per_year = trend_series(plot_df)

# -------------------------------------------------
# ECharts ë¹Œë” (KOR ë¼ë²¨ + í°íŠ¸ + íˆ´ë°•ìŠ¤ + ê¸°ì¤€ì„  + 1993â€“2010 í‰ê· ì„ )
# -------------------------------------------------
def build_line_chart(df, trend_df, theme, unit_label, unit_label_ko, avg_9310, show_markers=False):
    # ì „ì²´ ì›” ë²”ìœ„ë¡œ xì¶• êµ¬ì„±, ê° ì‹œë¦¬ì¦ˆëŠ” ëˆ„ë½ì›” Noneìœ¼ë¡œ

    # âœ… NaT ì•ˆì „ ê°€ë“œ ì¶”ê°€ (ë“¤ì—¬ì“°ê¸° ì£¼ì˜!)
    _d = pd.to_datetime(df["date"], errors="coerce").dropna()
    start = _d.min()
    end = _d.max()
    if pd.isna(start) or pd.isna(end) or start > end:
        # ë°ì´í„°ê°€ ë¹„ì •ìƒì´ë©´ ë¹ˆ ì°¨íŠ¸ ë°˜í™˜
        empty = Line(init_opts=opts.InitOpts(theme=theme, width="1000px", height="560px"))
        empty.add_xaxis([])
        return empty
    x = pd.period_range(start, end, freq="M").strftime("%Y-%m").tolist()

    color_map = {
        "ì¡°ìœ„ê³„ ì¬êµ¬ì„± (CSIRO)": "#ff7f50",
        "ìœ„ì„± ê³ ë„ê³„ (NOAA)": "#3b82f6",
        "ì¶”ì„¸ì„ ": "#10b981",
        "1993â€“2010 í‰ê· ": "#6366f1",  # indigo-ish
    }

    chart = Line(init_opts=opts.InitOpts(theme=theme, width="1000px", height="560px"))
    chart.add_xaxis(xaxis_data=x)

    for name, g in df.groupby("src"):
        g = g.sort_values("date").copy()
        g["xkey"] = g["date"].dt.strftime("%Y-%m")
        m = dict(zip(g["xkey"], g["value_adj"].round(2)))
        y_full = [m.get(xx, None) for xx in x]
        chart.add_yaxis(
            series_name=name,
            y_axis=y_full,
            is_smooth=True,
            symbol="circle",
            symbol_size=4,
            is_symbol_show=show_markers,
            is_connect_nones=False,
            linestyle_opts=opts.LineStyleOpts(width=3, opacity=0.95, color=color_map[name]),
            areastyle_opts=opts.AreaStyleOpts(opacity=0.18, color=color_map[name]),
        )

    # ì¶”ì„¸ì„ 
    if trend_df is not None and not trend_df.empty:
        trend_df = trend_df.sort_values("date").copy()
        trend_df["xkey"] = trend_df["date"].dt.strftime("%Y-%m")
        tmap = dict(zip(trend_df["xkey"], trend_df["trend"].round(2)))
        y_tr_full = [tmap.get(xx, None) for xx in x]
        chart.add_yaxis(
            series_name="ì¶”ì„¸ì„ ",
            y_axis=y_tr_full,
            is_smooth=False,
            symbol="none",
            is_connect_nones=False,
            linestyle_opts=opts.LineStyleOpts(width=2, type_="dashed", color=color_map["ì¶”ì„¸ì„ "]),
            areastyle_opts=opts.AreaStyleOpts(opacity=0),
        )

    # âœ… 1993â€“2010 í‰ê· ì„ (ìˆ˜í‰ì„ )
    chart.set_series_opts(
        markline_opts=opts.MarkLineOpts(
            data=[
                opts.MarkLineItem(y=0, name="ê¸°ì¤€ì„  (1900=0)"),
                opts.MarkLineItem(y=avg_9310, name="1993â€“2010 í‰ê· "),
            ],
            linestyle_opts=opts.LineStyleOpts(type_="dashed", opacity=0.6),
            label_opts=opts.LabelOpts(font_family="Pretendard")
        )
    )

    chart.set_global_opts(
        legend_opts=opts.LegendOpts(
            pos_top="2%", pos_left="center",
            textstyle_opts=opts.TextStyleOpts(font_family="Pretendard")
        ),
        tooltip_opts=opts.TooltipOpts(
            trigger="axis", axis_pointer_type="cross",
            value_formatter=f"{{value}} {unit_label_ko}"
        ),
        datazoom_opts=[opts.DataZoomOpts(type_="slider", range_start=0, range_end=100),
                       opts.DataZoomOpts(type_="inside")],
        xaxis_opts=opts.AxisOpts(
            type_="category", boundary_gap=False,
            axislabel_opts=opts.LabelOpts(font_family="Pretendard"),
            name_textstyle_opts=opts.TextStyleOpts(font_family="Pretendard"),
        ),
        yaxis_opts=opts.AxisOpts(
            name=f"ëˆ„ì  í•´ìˆ˜ë©´ ë³€í™” (1900=0, {unit_label_ko})",
            splitline_opts=opts.SplitLineOpts(is_show=True),
            axislabel_opts=opts.LabelOpts(font_family="Pretendard"),
            name_textstyle_opts=opts.TextStyleOpts(font_family="Pretendard"),
        ),
        toolbox_opts=opts.ToolboxOpts(
            is_show=True,
            feature=opts.ToolBoxFeatureOpts(
                save_as_image=opts.ToolBoxFeatureSaveAsImageOpts(title="PNG ì €ì¥"),
                restore=opts.ToolBoxFeatureRestoreOpts(),
                data_view=opts.ToolBoxFeatureDataViewOpts(is_show=False),
            )
        ),
    )
    return chart

chart = build_line_chart(plot_df[["date","src","value_adj"]],
                         trend_df, theme, unit_label, unit_label_ko,
                         avg_9310=avg_9310,
                         show_markers=show_markers)
st_pyecharts(chart, height="560px",
             key=f"echarts-{unit}-{year_range}-{smooth}-{show_markers}")

# -------------------------------------------------
# ë©”íŠ¸ë¦­ ì¹´ë“œ + í‘œ
# -------------------------------------------------
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("ê¸°ê°„", f"{year_range[0]}â€“{year_range[1]}")
with col2:
    st.metric(f"ë³€í™”ëŸ‰ ({unit_label_ko})", f"{(0 if np.isnan(change) else change):.1f}")
with col3:
    st.metric(f"ì—°í‰ê·  ìƒìŠ¹ë¥  ({unit_label_ko}/ë…„)", f"{(0 if np.isnan(rate) else rate):.2f}")
with col4:
    st.metric("ë°ì´í„° í¬ì¸íŠ¸", f"{len(plot_df):,}")

with st.expander("ğŸ§¾ ì›ìë£Œ ë¯¸ë¦¬ë³´ê¸°"):
    st.dataframe(
        plot_df[["date","src","value_adj"]]
        .rename(columns={"date":"ë‚ ì§œ","src":"ìë£Œì›","value_adj":f"í•´ìˆ˜ë©´({unit}, 1900=0)"})
        .reset_index(drop=True),
        use_container_width=True
    )

st.markdown("---")
st.caption("â“’ ë¯¸ë¦¼ë§ˆì´ìŠ¤í„°ê³  1í•™ë…„ 4ë°˜ 4ì¡° **ë§ˆìŒë°”ë‹¤ê±´ê°•ì¡°**")
